{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMtPzgYYjDP2ju/2C/K2g4c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":45,"metadata":{"id":"0AV_EbJF71N_","executionInfo":{"status":"ok","timestamp":1672165650640,"user_tz":-120,"elapsed":389,"user":{"displayName":"Diaa Ayman","userId":"16911330304019031940"}}},"outputs":[],"source":["import os\n","import time\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n"]},{"cell_type":"code","source":["os.getcwd()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"hFTOLW2k8bXq","executionInfo":{"status":"ok","timestamp":1672165650920,"user_tz":-120,"elapsed":4,"user":{"displayName":"Diaa Ayman","userId":"16911330304019031940"}},"outputId":"b9ae0ba0-587d-4781-d524-df49f5af40b1"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":[],"metadata":{"id":"qho0h6rq-DmD"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p079ax2S8bmd","executionInfo":{"status":"ok","timestamp":1672165652910,"user_tz":-120,"elapsed":1993,"user":{"displayName":"Diaa Ayman","userId":"16911330304019031940"}},"outputId":"7df5b9ec-1333-439e-a031-60f5f8b518da"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!unzip '/content/drive/MyDrive/ArASL_Database_54K_Final' -d'/content'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1q7ET-L8bo_","outputId":"9810de9b-be1d-4e39-e325-26350f46be5b"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Archive:  /content/drive/MyDrive/ArASL_Database_54K_Final.zip\n","replace /content/ArASL_Database_54K_Final/ain/AIN (1).JPG? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"markdown","source":["# New section"],"metadata":{"id":"d0GSQBBLBSPy"}},{"cell_type":"code","source":["!ls /content\n"],"metadata":{"id":"_oCpOtzb8brU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 128\n","\n","train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","    '/content/ArASL_Database_54K_Final', label_mode = train_labels, \n","    image_size = (64, 64), batch_size = batch_size\n",")\n","\n","train_dataset = train_dataset.map(lambda x: (x / 127.5) - 1)"],"metadata":{"id":"SmzQpae18bto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize = (12, 8))\n","\n","for images in train_dataset.take(1):\n","    for i in range(25):\n","        ax = plt.subplot(5, 5, i + 1)\n","        plt.imshow(images[i].numpy())\n","        plt.axis('off')"],"metadata":{"id":"_2BQU1xs8bwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for batch in train_dataset:\n","    plt.imshow(batch[0])\n","    print('Image_shape', batch[0].shape)\n","    break"],"metadata":{"id":"Ad8T7YMa8byO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_weights_kernel = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.02)\n"],"metadata":{"id":"6WSs6k6T8b1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generator_model():\n","    model = tf.keras.Sequential()\n","\n","    model.add(layers.Input(shape = (100, )))\n","    model.add(layers.Dense(4 * 4 * 256))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","    model.add(layers.Reshape((4, 4, 256)))\n","    \n","    model.add(layers.Conv2DTranspose(128, kernel_size = 4, padding = 'same', strides = 2,\n","                                     kernel_initializer = init_weights_kernel , use_bias = False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","    \n","    model.add(layers.Conv2DTranspose(64, kernel_size = 4, padding = 'same', strides = 2,\n","                                     kernel_initializer = init_weights_kernel, use_bias = False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","    \n","    model.add(layers.Conv2DTranspose(32, kernel_size = 4, padding = 'same', strides = 2,\n","                                     kernel_initializer = init_weights_kernel, use_bias = False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","\n","    model.add(layers.Conv2DTranspose(3, kernel_size = 4, padding = 'same', strides = 2, activation = 'tanh',\n","                                     kernel_initializer = init_weights_kernel, use_bias = False))\n","   \n","    \n","    return model"],"metadata":{"id":"cZL4QOErBrCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = generator_model()\n","\n","generator.summary()"],"metadata":{"id":"0SY4orYTBtx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["noise = tf.random.normal([1, 100])\n","\n","generated_image = generator(noise, training = False)\n","\n","generated_image.shape"],"metadata":{"id":"fd0v8tzeBvXr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_image[0, :5, :5] \n"],"metadata":{"id":"TTcRwjfJBxvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow((generated_image[0, :, :, :] * 255 * 100))\n"],"metadata":{"id":"SL7i3KHRBzCk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def discriminator_model():\n","    model = tf.keras.Sequential()\n","\n","    model.add(layers.Conv2D(32, kernel_size = 3, strides = 2, input_shape = (64, 64, 3), padding = 'same',\n","                            kernel_initializer = init_weights_kernel, use_bias = False ))\n","    model.add(layers.LeakyReLU(alpha = 0.2))\n","    model.add(layers.Dropout(0.3))\n","    \n","    model.add(layers.Conv2D(64, kernel_size = 3, strides = 2, padding = 'same',\n","                            kernel_initializer = init_weights_kernel, use_bias = False))\n","    model.add(layers.ZeroPadding2D(padding = ((0, 1), (0, 1))))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(alpha = 0.2))\n","    model.add(layers.Dropout(0.25))\n","    \n","    model.add(layers.Conv2D(128, kernel_size = 3, strides = 2, padding = 'same',\n","                            kernel_initializer = init_weights_kernel, use_bias = False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(alpha = 0.2))\n","    model.add(layers.Dropout(0.3))\n","    \n","    model.add(layers.Conv2D(256, kernel_size = 3, strides=1, padding = 'same',\n","                            kernel_initializer = init_weights_kernel, use_bias = False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(alpha = 0.2))\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(1, activation = 'sigmoid'))\n","\n","    return model"],"metadata":{"id":"xOtsoTVKB05M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["discriminator = discriminator_model()\n","\n","discriminator.summary()"],"metadata":{"id":"AIIUeg7dB45b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = discriminator(generated_image)\n","print (output)"],"metadata":{"id":"SbdDJBshB6q9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bce = tf.keras.losses.BinaryCrossentropy()\n"],"metadata":{"id":"wAny1LHOB90T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def discriminator_loss(real_output, fake_output):\n","    real_loss = bce(tf.ones_like(real_output), real_output)\n","    fake_loss = bce(tf.zeros_like(fake_output), fake_output)\n","\n","    total_loss = real_loss + fake_loss\n","\n","    return total_loss"],"metadata":{"id":"3xLZnAv1B_He"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generator_loss(fake_output):\n","    gen_loss = bce(tf.ones_like(fake_output), fake_output)\n","\n","    return gen_loss"],"metadata":{"id":"Vz7zKgOxCA_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5)\n","discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5)"],"metadata":{"id":"kdFIQKoPCDN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n","checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer,\n","                                 discriminator_optimizer = discriminator_optimizer,\n","                                 generator = generator,\n","                                 discriminator = discriminator)"],"metadata":{"id":"J86hkpvlCLdo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 50\n","noise_dim = 100\n","num_examples_to_generate = 16\n","\n","seed = tf.random.normal([num_examples_to_generate, noise_dim])"],"metadata":{"id":"dLDjFAdrCPJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_and_plot_images(model, epoch, test_input):\n","  \n","    predictions = model(test_input, training = False)\n","\n","    fig = plt.figure(figsize = (8, 4))\n","\n","    for i in range(predictions.shape[0]):\n","        plt.subplot(4, 4, i + 1)  \n","        plt.imshow((predictions[i, :, :, :] * 0.5 + 0.5))\n","        plt.axis('off')\n","\n","    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n","    plt.show()"],"metadata":{"id":"hcg6dfCOCROc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(images):\n","    noise = tf.random.normal([batch_size, noise_dim])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = generator(noise, training = True)\n","\n","        real_output = discriminator(images, training = True)\n","        fake_output = discriminator(generated_images, training = True)\n","\n","        gen_loss = generator_loss(fake_output)\n","        disc_loss = discriminator_loss(real_output, fake_output)\n","        \n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","    return (gen_loss, disc_loss, tf.reduce_mean(real_output), tf.reduce_mean(fake_output))"],"metadata":{"id":"dFWfMtEpCS1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(images):\n","    noise = tf.random.normal([batch_size, noise_dim])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = generator(noise, training = True)\n","\n","        real_output = discriminator(images, training = True)\n","        fake_output = discriminator(generated_images, training = True)\n","\n","        gen_loss = generator_loss(fake_output)\n","        disc_loss = discriminator_loss(real_output, fake_output)\n","        \n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","    return (gen_loss, disc_loss, tf.reduce_mean(real_output), tf.reduce_mean(fake_output))"],"metadata":{"id":"DQMeHuLxCU4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(dataset, epochs):\n","    \n","    gen_loss_list = []\n","    disc_loss_list = []\n","    \n","    real_score_list = []\n","    fake_score_list = []\n","\n","    for epoch in tqdm(range(epochs)):\n","        start = time.time()\n","        num_batches = len(dataset)\n","        \n","        print(f'Training started with  epoch {epoch + 1} with {num_batches} batches..')\n","        \n","        total_gen_loss = 0\n","        total_disc_loss = 0\n","        \n","        for batch in dataset:\n","            generator_loss, discriminator_loss, real_score, fake_score = train_step(batch)\n","            \n","            total_gen_loss += generator_loss\n","            total_disc_loss += discriminator_loss\n","        \n","        mean_gen_loss = total_gen_loss / num_batches\n","        mean_disc_loss = total_disc_loss / num_batches\n","           \n","        print('Losses after epoch %5d: generator %.3f, discriminator %.3f,\\\n","               real_score %.2f%%, fake_score %.2f%%'  %\n","              (epoch+1, generator_loss, discriminator_loss, real_score * 100, fake_score * 100))\n","\n","        generate_and_plot_images(generator, epoch + 1, seed)                 \n","\n","        gen_loss_list.append(mean_gen_loss)\n","        disc_loss_list.append(mean_disc_loss)\n","        real_score_list.append(real_score)\n","        fake_score_list.append(fake_score)\n","\n","        if (epoch + 1) % 10 == 0:\n","              checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))\n","\n","  \n","    return  gen_loss_list, disc_loss_list, real_score_list, fake_score_list "],"metadata":{"id":"xlsSEY4dCX6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gen_loss_epochs, disc_loss_epochs, real_score_list, fake_score_list = train(train_dataset, epochs = epochs)\n"],"metadata":{"id":"YL8ppTVECe6X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, (ax1,ax2) = plt.subplots(1, 2, figsize = (12, 8))\n","\n","ax1.plot(gen_loss_epochs, label = 'Generator loss', alpha = 0.5)\n","ax1.plot(disc_loss_epochs, label = 'Discriminator loss', alpha = 0.5)\n","ax1.legend()\n","ax1.set_title('Training Losses')\n","\n","ax2.plot(real_score_list, label = 'Real_score', alpha = 0.5)\n","ax2.plot(fake_score_list, label = 'Fake_score', alpha = 0.5)\n","ax2.set_title('Accuracy Scores')\n","\n","ax2.legend()"],"metadata":{"id":"Tp08mIqoCtef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["discriminator_model.save"],"metadata":{"id":"02Cqy_CKCwb_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#discriminator_model.save()\n","discriminator_model().save('discriminator_model.h5')\n"],"metadata":{"id":"7MLB_m97rtif"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NkevonUsr_Sf"},"execution_count":null,"outputs":[]}]}